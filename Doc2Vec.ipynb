{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "368d2bc4-c114-4555-81d3-66fbd0a0b2f9",
   "metadata": {},
   "source": [
    "### Explanation of Doc2Vec\n",
    "\n",
    "**Doc2Vec** is an extension of Word2Vec that generates vector representations for entire documents or pieces of text, rather than just individual words. It was introduced by Quoc Le and Tomas Mikolov in 2014 and is useful for tasks where understanding the meaning of entire documents, paragraphs, or sentences is essential.\n",
    "\n",
    "### How Doc2Vec Works\n",
    "\n",
    "Doc2Vec extends Word2Vec by adding a new vector (document ID vector) for each document. These vectors are trained to predict words in the document, similar to how Word2Vec uses context words to predict a target word. There are two main approaches to training Doc2Vec models: Distributed Memory (DM) and Distributed Bag of Words (DBOW).\n",
    "\n",
    "#### 1. Distributed Memory (DM)\n",
    "\n",
    "- **Objective**: Predict a target word using the context words and the document vector.\n",
    "- **Approach**: Similar to CBOW in Word2Vec. Both context words and the document vector are used to predict the target word.\n",
    "- **Advantages**: Tends to perform well in capturing the semantic meaning of documents.\n",
    "\n",
    "**Diagram**:\n",
    "```\n",
    "[Document ID] + [Context Words] -> Neural Network -> [Target Word]\n",
    "```\n",
    "\n",
    "#### 2. Distributed Bag of Words (DBOW)\n",
    "\n",
    "- **Objective**: Predict context words using the document vector.\n",
    "- **Approach**: Similar to Skip-gram in Word2Vec. The document vector alone is used to predict multiple context words in the document.\n",
    "- **Advantages**: Computationally simpler and faster to train.\n",
    "\n",
    "**Diagram**:\n",
    "```\n",
    "[Document ID] -> Neural Network -> [Context Words]\n",
    "```\n",
    "\n",
    "### Example Code Using Gensim\n",
    "\n",
    "Here's how you can train and use Doc2Vec models using the Gensim library:\n",
    "\n",
    "#### Installation\n",
    "\n",
    "First, install Gensim if you haven't already:\n",
    "```bash\n",
    "pip install gensim\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b458e0ba-b54d-4bd3-a974-8ab96a5231a4",
   "metadata": {},
   "source": [
    "# Training a Doc2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7549351e-60ef-4be9-9352-cc187cc1270d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['i', 'love', 'machine', 'learning', '.', 'it', \"'s\", 'fascinating', '.'], tags=['0']),\n",
       " TaggedDocument(words=['deep', 'learning', 'is', 'a', 'subset', 'of', 'machine', 'learning', '.'], tags=['1']),\n",
       " TaggedDocument(words=['natural', 'language', 'processing', 'is', 'a', 'key', 'area', 'of', 'ai', '.'], tags=['2']),\n",
       " TaggedDocument(words=['gensim', 'is', 'a', 'library', 'for', 'topic', 'modeling', 'and', 'document', 'indexing', '.'], tags=['3']),\n",
       " TaggedDocument(words=['doc2vec', 'is', 'an', 'extension', 'of', 'word2vec', '.'], tags=['4'])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"I love machine learning. It's fascinating.\",\n",
    "    \"Deep learning is a subset of machine learning.\",\n",
    "    \"Natural language processing is a key area of AI.\",\n",
    "    \"Gensim is a library for topic modeling and document indexing.\",\n",
    "    \"Doc2Vec is an extension of Word2Vec.\"\n",
    "]\n",
    "\n",
    "# Tokenize and tag the documents\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(doc.lower()), tags=[str(i)]) for i, doc in enumerate(documents)]\n",
    "tagged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c288ed2a-c821-4d52-8ca0-56a2a93aee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the Doc2Vec model\n",
    "model = Doc2Vec(vector_size=50, alpha=0.025, min_alpha=0.00025, min_count=1, dm=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06185d52-3d52-48ee-b976-6b2a42b1ff65",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "   - `vector_size=50`: The size of the vectors.\n",
    "   - `alpha=0.025`: The initial learning rate.\n",
    "   - `min_alpha=0.00025`: The minimum learning rate.\n",
    "   - `min_count=1`: Ignores all words with total frequency lower than this.\n",
    "   - `dm=1`: Indicates the use of the Distributed Memory (DM) model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "523d2af7-ab66-48f8-b8ed-9220e4aa79fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "# Train the model\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c27709be-a75d-4e5d-bda5-92b8695632fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(\"d2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a49af611-8423-4740-82ad-5bfd157987d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = Doc2Vec.load(\"d2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49c35521-1e51-4bfe-b2ed-33445fd744b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for new document:\n",
      "[-0.04624398  0.00421499 -0.01397642 -0.01604808 -0.00906577 -0.00510722\n",
      "  0.00076339  0.0377649  -0.03776836  0.01884614  0.02258971  0.01798172\n",
      "  0.0033154  -0.03847247  0.02716012 -0.02880351 -0.00393291  0.00821783\n",
      " -0.05536242 -0.03861832 -0.0200435   0.01773427  0.0063881   0.02096928\n",
      " -0.00288691  0.00093806 -0.02351245 -0.01838718 -0.00294462 -0.02224243\n",
      "  0.02393194  0.01633536 -0.02707224 -0.01975657 -0.032386    0.01691411\n",
      "  0.00171951 -0.01972627  0.00995371 -0.01926793  0.00454859  0.03199244\n",
      "  0.0116352  -0.02720493 -0.00694919  0.00993334  0.02305961 -0.00091604\n",
      "  0.02895818 -0.01197604]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Infer a vector for a new document\n",
    "new_doc = \"I enjoy learning about AI and machine learning.\"\n",
    "new_vector = model.infer_vector(word_tokenize(new_doc.lower()))\n",
    "print(f\"Vector for new document:\\n{new_vector}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29cd2b75-e597-4661-97e7-d8b308e07cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar documents:\n",
      "[('3', 0.9131336808204651), ('0', 0.8961871862411499), ('1', 0.8642350435256958), ('2', 0.8620454668998718), ('4', 0.7248851656913757)]\n"
     ]
    }
   ],
   "source": [
    "# Find similar documents\n",
    "similar_docs = model.dv.most_similar([new_vector])\n",
    "print(f\"Most similar documents:\\n{similar_docs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf46b206-b194-4d93-a9eb-078630a337de",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
