{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a361034c-bc53-4a2f-9a8e-f4d79d4f24ec",
   "metadata": {},
   "source": [
    "### Examples of Unigram, Bigram, and Trigram Tokens\n",
    "\n",
    "Let's take a long sentence as an example:\n",
    "\"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "#### Unigram Tokens\n",
    "Unigrams are single words. The sentence tokenized into unigrams would be:\n",
    "- \"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"\n",
    "\n",
    "#### Bigram Tokens\n",
    "Bigrams are pairs of consecutive words. The sentence tokenized into bigrams would be:\n",
    "- \"The quick\", \"quick brown\", \"brown fox\", \"fox jumps\", \"jumps over\", \"over the\", \"the lazy\", \"lazy dog\"\n",
    "\n",
    "#### Trigram Tokens\n",
    "Trigrams are triples of consecutive words. The sentence tokenized into trigrams would be:\n",
    "- \"The quick brown\", \"quick brown fox\", \"brown fox jumps\", \"fox jumps over\", \"jumps over the\", \"over the lazy\", \"the lazy dog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e9f8b4-96d8-4726-922d-c06743a7cef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23337255-7a53-4bf6-8e93-5fa5d9c4fa92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709b71fb-6932-4dc6-8540-fafc8a2430a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce53c5a6-35e3-4070-86df-96420a38476f",
   "metadata": {},
   "source": [
    "### What is a Corpus?\n",
    "\n",
    "A corpus (plural: corpora) is a large and structured set of texts (documents). In the context of natural language processing (NLP) and text mining, a corpus is used to train and evaluate models. It can be a collection of written texts, transcriptions of spoken words, or any other form of linguistic data.\n",
    "\n",
    "Example of a corpus:\n",
    "- Document 1: \"The quick brown fox jumps over the lazy dog.\"\n",
    "- Document 2: \"The dog barks loudly.\"\n",
    "- Document 3: \"Foxes are clever animals.\"\n",
    "\n",
    "### What is a Vocabulary in the Above Context?\n",
    "\n",
    "The vocabulary of a corpus is the set of all unique words or tokens that appear in the corpus. It represents the lexicon used in the documents of the corpus. \n",
    "\n",
    "For the example corpus given above, the vocabulary would include all unique words from the documents:\n",
    "\n",
    "- \"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\", \"barks\", \"loudly\", \"Foxes\", \"are\", \"clever\", \"animals\"\n",
    "\n",
    "To illustrate further, letâ€™s build a simple vocabulary and token sets:\n",
    "\n",
    "### Example Corpus\n",
    "\n",
    "1. Document 1: \"The quick brown fox jumps over the lazy dog.\"\n",
    "2. Document 2: \"The dog barks loudly.\"\n",
    "3. Document 3: \"Foxes are clever animals.\"\n",
    "\n",
    "#### Vocabulary\n",
    "The set of all unique words in the corpus:\n",
    "- {\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\", \"barks\", \"loudly\", \"Foxes\", \"are\", \"clever\", \"animals\"}\n",
    "\n",
    "### Tokenization Examples from Document 1\n",
    "\n",
    "#### Unigrams\n",
    "- [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
    "\n",
    "#### Bigrams\n",
    "- [\"The quick\", \"quick brown\", \"brown fox\", \"fox jumps\", \"jumps over\", \"over the\", \"the lazy\", \"lazy dog\"]\n",
    "\n",
    "#### Trigrams\n",
    "- [\"The quick brown\", \"quick brown fox\", \"brown fox jumps\", \"fox jumps over\", \"jumps over the\", \"over the lazy\", \"the lazy dog\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fd7889-d838-4ba1-bf23-d55a18bc7674",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b98a407a-a050-48ad-821b-7de27a4477fe",
   "metadata": {},
   "source": [
    "# Tokenization Techniques in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d0ce47-88af-47ec-8038-0015c752df93",
   "metadata": {},
   "source": [
    "Tokens in natural language processing (NLP) are the individual elements that make up a text, such as words, subwords, or characters. There are various ways to define and extract tokens depending on the application and the complexity of the language model. Here are several common methods:\n",
    "\n",
    "### 1. Word Tokenization\n",
    "\n",
    "**Definition**: Splitting text into individual words.\n",
    "\n",
    "**Example**:\n",
    "- Sentence: \"The quick brown fox jumps over the lazy dog.\"\n",
    "- Tokens: [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
    "\n",
    "**Applications**: General-purpose text processing, information retrieval, and traditional NLP tasks.\n",
    "\n",
    "### 2. Subword Tokenization\n",
    "\n",
    "**Definition**: Splitting words into smaller subword units, which can include prefixes, suffixes, or common word parts. This is useful for handling out-of-vocabulary words.\n",
    "\n",
    "**Example**:\n",
    "- Sentence: \"unhappiness\"\n",
    "- Tokens: [\"un\", \"happi\", \"ness\"]\n",
    "\n",
    "**Applications**: Neural machine translation, language modeling (e.g., Byte Pair Encoding (BPE), WordPiece).\n",
    "\n",
    "### 3. Character Tokenization\n",
    "\n",
    "**Definition**: Splitting text into individual characters.\n",
    "\n",
    "**Example**:\n",
    "- Sentence: \"cat\"\n",
    "- Tokens: [\"c\", \"a\", \"t\"]\n",
    "\n",
    "**Applications**: Languages with complex morphology, tasks requiring fine-grained text analysis, and character-level language models.\n",
    "\n",
    "### 4. N-gram Tokenization\n",
    "\n",
    "**Definition**: Extracting contiguous sequences of n items (words, subwords, or characters).\n",
    "\n",
    "**Example**:\n",
    "- Sentence: \"The quick brown fox\"\n",
    "- Unigrams: [\"The\", \"quick\", \"brown\", \"fox\"]\n",
    "- Bigrams: [\"The quick\", \"quick brown\", \"brown fox\"]\n",
    "- Trigrams: [\"The quick brown\", \"quick brown fox\"]\n",
    "\n",
    "**Applications**: Text classification, information retrieval, and feature extraction for machine learning models.\n",
    "\n",
    "### 5. Sentence Tokenization\n",
    "\n",
    "**Definition**: Splitting text into individual sentences.\n",
    "\n",
    "**Example**:\n",
    "- Text: \"Hello world! How are you?\"\n",
    "- Tokens: [\"Hello world!\", \"How are you?\"]\n",
    "\n",
    "**Applications**: Document summarization, sentiment analysis, and text segmentation.\n",
    "\n",
    "### 6. Byte-Pair Encoding (BPE)\n",
    "\n",
    "**Definition**: An algorithm that iteratively merges the most frequent pairs of bytes or characters in the text, resulting in subword units.\n",
    "\n",
    "**Example**:\n",
    "- Sentence: \"low lower lowest\"\n",
    "- Tokens: [\"low\", \"low\", \"er\", \"low\", \"est\"]\n",
    "\n",
    "**Applications**: Machine translation, language modeling, and reducing the vocabulary size.\n",
    "\n",
    "### 7. WordPiece Tokenization\n",
    "\n",
    "**Definition**: Similar to BPE but uses a probabilistic approach to find the best subword units.\n",
    "\n",
    "**Example**:\n",
    "- Sentence: \"unhappiness\"\n",
    "- Tokens: [\"un\", \"##happiness\"]\n",
    "\n",
    "**Applications**: Google's BERT and other Transformer-based models.\n",
    "\n",
    "### 8. Regular Expression Tokenization\n",
    "\n",
    "**Definition**: Using regular expressions to define patterns for splitting text into tokens.\n",
    "\n",
    "**Example**:\n",
    "- Sentence: \"Email: user@example.com\"\n",
    "- Pattern: r'\\w+|\\S'\n",
    "- Tokens: [\"Email\", \":\", \"user\", \"@\", \"example\", \".\", \"com\"]\n",
    "\n",
    "**Applications**: Custom text processing tasks, specific tokenization rules, and handling special text formats.\n",
    "\n",
    "### 9. Morpheme-based Tokenization\n",
    "\n",
    "**Definition**: Splitting text into morphemes, the smallest meaningful units in a language.\n",
    "\n",
    "**Example**:\n",
    "- Word: \"unhappiness\"\n",
    "- Tokens: [\"un-\", \"happy\", \"-ness\"]\n",
    "\n",
    "**Applications**: Linguistic research, languages with complex morphology, and text analysis.\n",
    "\n",
    "### 10. Semantic Tokenization\n",
    "\n",
    "**Definition**: Splitting text into units based on semantic meaning, such as named entities or phrases.\n",
    "\n",
    "**Example**:\n",
    "- Sentence: \"Barack Obama was born in Hawaii.\"\n",
    "- Tokens: [\"Barack Obama\", \"was born in\", \"Hawaii\"]\n",
    "\n",
    "**Applications**: Named entity recognition (NER), information extraction, and semantic analysis.\n",
    "\n",
    "### Choosing the Right Tokenization Method\n",
    "\n",
    "The choice of tokenization method depends on the specific task and the characteristics of the text data. For instance:\n",
    "\n",
    "- **Word Tokenization** is simple and effective for many general-purpose NLP tasks.\n",
    "- **Subword and Character Tokenization** are better suited for languages with rich morphology or for models dealing with out-of-vocabulary words.\n",
    "- **N-gram Tokenization** can capture context and sequential information.\n",
    "- **Sentence Tokenization** is useful for tasks involving sentence-level analysis.\n",
    "- **BPE and WordPiece Tokenization** are commonly used in modern NLP models like BERT and GPT.\n",
    "- **Regular Expression Tokenization** allows for custom tokenization based on specific patterns.\n",
    "- **Morpheme-based and Semantic Tokenization** are more advanced and used for specific linguistic or semantic tasks.\n",
    "\n",
    "Each method has its advantages and trade-offs, and the right choice often depends on the language, the corpus, and the specific application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38189108-5270-4e84-aa7a-9e82027a0bab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
