{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23e29e7f-4bd5-4409-ba46-8e80e14dcba4",
   "metadata": {},
   "source": [
    "# What is CountVectorizer?\n",
    "\n",
    "`CountVectorizer` is a tool provided by libraries such as scikit-learn in Python. It converts a collection of text documents into a matrix of token counts. Essentially, it creates a vocabulary of all the unique words in the documents and counts the occurrences of each word.\n",
    "\n",
    "### Example\n",
    "\n",
    "Suppose we have the following three simple documents:\n",
    "\n",
    "1. \"The cat sat on the mat\"\n",
    "2. \"The dog sat on the log\"\n",
    "3. \"The cat and the dog\"\n",
    "\n",
    "#### Step-by-Step Process\n",
    "\n",
    "1. **Tokenization**:\n",
    "   - Split each document into words (tokens).\n",
    "   \n",
    "   Document 1: `[\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]`\n",
    "   Document 2: `[\"The\", \"dog\", \"sat\", \"on\", \"the\", \"log\"]`\n",
    "   Document 3: `[\"The\", \"cat\", \"and\", \"the\", \"dog\"]`\n",
    "\n",
    "2. **Creating the Vocabulary**:\n",
    "   - Identify all unique words in the documents.\n",
    "   \n",
    "   Vocabulary: `[\"and\", \"cat\", \"dog\", \"log\", \"mat\", \"on\", \"sat\", \"the\"]`\n",
    "   \n",
    "3. **Creating the Count Matrix**:\n",
    "   - For each document, count the occurrences of each word in the vocabulary.\n",
    "\n",
    "   |    | and | cat | dog | log | mat | on | sat | the |\n",
    "   |----|-----|-----|-----|-----|-----|----|-----|-----|\n",
    "   | D1 |  0  |  1  |  0  |  0  |  1  | 1  |  1  |  2  |\n",
    "   | D2 |  0  |  0  |  1  |  1  |  0  | 1  |  1  |  2  |\n",
    "   | D3 |  1  |  1  |  1  |  0  |  0  | 0  |  0  |  2  |\n",
    "\n",
    "   Each row in the matrix corresponds to a document, and each column corresponds to a word in the vocabulary. The values indicate the count of each word in each document.\n",
    "\n",
    "- The word \"the\" appears twice in each document, so the count for \"the\" is 2 in all rows.\n",
    "- The word \"cat\" appears once in the first and third documents but not in the second, hence the counts `1, 0, 1` for \"cat\".\n",
    "- The word \"dog\" appears once in the second and third documents but not in the first, hence the counts `0, 1, 1` for \"dog\".\n",
    "- Similarly, other words are counted based on their occurrences in each document.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8b6eca-6f95-4f1d-8efd-cdba81445d35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac23df9-99a6-4524-988c-808311955e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c90ebf09-00b8-45e5-861a-b3607153446f",
   "metadata": {},
   "source": [
    "# TF-IDF Vectorizer\n",
    "\n",
    "TF-IDF is a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents (corpus). It is the product of two statistics, Term Frequency (TF) and Inverse Document Frequency (IDF).\n",
    "\n",
    "1. **Term Frequency (TF)**:\n",
    "   - Measures how frequently a term appears in a document.\n",
    "   - Formula: $ \\text{TF}(t,d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d} $\n",
    "\n",
    "2. **Inverse Document Frequency (IDF)**:\n",
    "   - Measures how important a term is in the entire corpus.\n",
    "   - Formula: $ \\text{IDF}(t) = \\log\\left(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing term } t}\\right) $\n",
    "\n",
    "3. **TF-IDF**:\n",
    "   - Combines TF and IDF to give a weight for each term in each document.\n",
    "   - Formula: $ \\text{TF-IDF}(t,d) = \\text{TF}(t,d) \\times \\text{IDF}(t) $\n",
    "\n",
    "### Example Documents\n",
    "\n",
    "1. Document 1 (D1): \"The cat sat on the mat\"\n",
    "2. Document 2 (D2): \"The dog sat on the log\"\n",
    "3. Document 3 (D3): \"The cat and the dog\"\n",
    "\n",
    "### Step-by-Step Calculation\n",
    "\n",
    "#### Step 1: Calculate Term Frequency (TF)\n",
    "\n",
    "For each term in each document:\n",
    "\n",
    "**Document 1 (D1):**\n",
    "- TF(\"the\", D1) = 2/6 = 0.333\n",
    "- TF(\"cat\", D1) = 1/6 = 0.167\n",
    "- TF(\"sat\", D1) = 1/6 = 0.167\n",
    "- TF(\"on\", D1) = 1/6 = 0.167\n",
    "- TF(\"mat\", D1) = 1/6 = 0.167\n",
    "\n",
    "**Document 2 (D2):**\n",
    "- TF(\"the\", D2) = 2/6 = 0.333\n",
    "- TF(\"dog\", D2) = 1/6 = 0.167\n",
    "- TF(\"sat\", D2) = 1/6 = 0.167\n",
    "- TF(\"on\", D2) = 1/6 = 0.167\n",
    "- TF(\"log\", D2) = 1/6 = 0.167\n",
    "\n",
    "**Document 3 (D3):**\n",
    "- TF(\"the\", D3) = 2/5 = 0.4\n",
    "- TF(\"cat\", D3) = 1/5 = 0.2\n",
    "- TF(\"and\", D3) = 1/5 = 0.2\n",
    "- TF(\"dog\", D3) = 1/5 = 0.2\n",
    "\n",
    "#### Step 2: Calculate Inverse Document Frequency (IDF)\n",
    "\n",
    "For each term in the vocabulary:\n",
    "- IDF(\"the\") = log(3/3) = log(1) = 0\n",
    "- IDF(\"cat\") = log(3/2) = 0.176\n",
    "- IDF(\"sat\") = log(3/2) = 0.176\n",
    "- IDF(\"on\") = log(3/2) = 0.176\n",
    "- IDF(\"mat\") = log(3/1) = 0.477\n",
    "- IDF(\"dog\") = log(3/2) = 0.176\n",
    "- IDF(\"log\") = log(3/1) = 0.477\n",
    "- IDF(\"and\") = log(3/1) = 0.477\n",
    "\n",
    "#### Step 3: Calculate TF-IDF\n",
    "\n",
    "Multiply the TF by the corresponding IDF for each term in each document.\n",
    "\n",
    "**Document 1 (D1):**\n",
    "- TF-IDF(\"the\", D1) = 0.333 * 0 = 0\n",
    "- TF-IDF(\"cat\", D1) = 0.167 * 0.176 = 0.029\n",
    "- TF-IDF(\"sat\", D1) = 0.167 * 0.176 = 0.029\n",
    "- TF-IDF(\"on\", D1) = 0.167 * 0.176 = 0.029\n",
    "- TF-IDF(\"mat\", D1) = 0.167 * 0.477 = 0.080\n",
    "\n",
    "**Document 2 (D2):**\n",
    "- TF-IDF(\"the\", D2) = 0.333 * 0 = 0\n",
    "- TF-IDF(\"dog\", D2) = 0.167 * 0.176 = 0.029\n",
    "- TF-IDF(\"sat\", D2) = 0.167 * 0.176 = 0.029\n",
    "- TF-IDF(\"on\", D2) = 0.167 * 0.176 = 0.029\n",
    "- TF-IDF(\"log\", D2) = 0.167 * 0.477 = 0.080\n",
    "\n",
    "**Document 3 (D3):**\n",
    "- TF-IDF(\"the\", D3) = 0.4 * 0 = 0\n",
    "- TF-IDF(\"cat\", D3) = 0.2 * 0.176 = 0.035\n",
    "- TF-IDF(\"and\", D3) = 0.2 * 0.477 = 0.095\n",
    "- TF-IDF(\"dog\", D3) = 0.2 * 0.176 = 0.035\n",
    "\n",
    "#### Summary Table\n",
    "\n",
    "|    | and  | cat  | dog  | log  | mat  | on   | sat  | the |\n",
    "|----|------|------|------|------|------|------|------|-----|\n",
    "| D1 | 0    | 0.029| 0    | 0    | 0.080| 0.029| 0.029| 0   |\n",
    "| D2 | 0    | 0    | 0.029| 0.080| 0    | 0.029| 0.029| 0   |\n",
    "| D3 | 0.095| 0.035| 0.035| 0    | 0    | 0    | 0    | 0   |\n",
    "\n",
    "\n",
    "- The values in the DataFrame represent the TF-IDF scores for each term in each document.\n",
    "- Higher scores indicate higher importance of the term in the respective document relative to the corpus.\n",
    "\n",
    "By using TF-IDF, we can better understand the significance of words in documents, which helps in improving the performance and accuracy of machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37369bf1-ea76-4926-a495-87911231b049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617a3a39-13af-4faa-b37a-964a14f70ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28ec5815-d6fc-424d-9448-453232cf08dd",
   "metadata": {},
   "source": [
    "# Limitations of Bag of Words Models\n",
    "\n",
    "1. **Ignoring Word Order**:\n",
    "   - BoW models do not capture the order of words in the text. This means they miss out on the syntactic and semantic relationships between words, which can be crucial for understanding the context and meaning.\n",
    "\n",
    "2. **Ignoring Context**:\n",
    "   - These models treat each word independently and ignore the context in which words appear. For example, the words \"bank\" in \"river bank\" and \"bank\" in \"savings bank\" are treated the same.\n",
    "\n",
    "3. **High Dimensionality**:\n",
    "   - The feature space can become very large, especially for large corpora with extensive vocabularies. This leads to high computational costs and increased memory usage.\n",
    "\n",
    "4. **Sparse Matrices**:\n",
    "   - The resulting matrices are often sparse, with many zero entries, since most words do not appear in most documents. Sparse matrices can be computationally expensive to process.\n",
    "\n",
    "5. **Lack of Semantic Understanding**:\n",
    "   - BoW models do not capture the meaning of words. Words with similar meanings (synonyms) or different forms of the same word (stemming, lemmatization) are treated as distinct features.\n",
    "\n",
    "6. **Fixed Vocabulary**:\n",
    "   - The vocabulary is fixed at the time of model training. New words that appear in future documents but were not present in the training set are not handled well.\n",
    "\n",
    "7. **Sensitivity to Frequent Words**:\n",
    "   - Common words (stop words) can dominate the feature space if not removed, potentially drowning out less frequent but more informative terms.\n",
    "\n",
    "8. **IDF Sensitivity to Rare Words**:\n",
    "   - While TF-IDF reduces the impact of frequent words, it can overly amplify the importance of rare words that may not be relevant.\n",
    "\n",
    "### Example of Limitations\n",
    "\n",
    "Consider the sentences:\n",
    "1. \"The cat sat on the mat.\"\n",
    "2. \"The mat sat on the cat.\"\n",
    "\n",
    "BoW models will treat these sentences as identical because they have the same words with the same frequencies, despite having different meanings due to word order.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73976e46-8efb-40eb-9d53-c57fc4c5d0ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "739d6ebf-9b31-418f-92a2-42ac9ef89e66",
   "metadata": {},
   "source": [
    "Feature extraction using word embedding models like Word2Vec, GloVe, and BERT helps overcome the limitations of Bag of Words (BoW) models in several ways. Here’s how these embeddings improve upon BoW models and their additional advantages:\n",
    "\n",
    "### Overcoming Limitations of BoW Models\n",
    "\n",
    "1. **Capturing Semantic Meaning**:\n",
    "   - **BoW Limitation**: BoW models treat each word independently and ignore semantic similarities.\n",
    "   - **Embeddings Solution**: Word embeddings represent words in a continuous vector space where semantically similar words are mapped to nearby points. This allows the model to understand that \"king\" and \"queen\" are related or that \"bank\" in \"river bank\" and \"savings bank\" have different contexts.\n",
    "\n",
    "2. **Contextual Understanding**:\n",
    "   - **BoW Limitation**: BoW and TF-IDF ignore the context in which words appear.\n",
    "   - **Embeddings Solution**: Contextualized embeddings like BERT create different vectors for the same word depending on its context, capturing the nuanced meaning of words based on their surrounding text.\n",
    "\n",
    "3. **Handling Synonyms and Polysemy**:\n",
    "   - **BoW Limitation**: Synonyms are treated as different features, and polysemous words (words with multiple meanings) are treated the same.\n",
    "   - **Embeddings Solution**: Embeddings capture the relationships between synonyms and disambiguate polysemous words by considering context (especially with models like BERT).\n",
    "\n",
    "4. **Dimensionality Reduction**:\n",
    "   - **BoW Limitation**: BoW models result in very high-dimensional sparse vectors.\n",
    "   - **Embeddings Solution**: Word embeddings produce dense vectors of fixed, much lower dimensions, reducing the computational load and making the feature space more manageable.\n",
    "\n",
    "5. **Fixed Vocabulary**:\n",
    "   - **BoW Limitation**: New words not seen during training are not handled well.\n",
    "   - **Embeddings Solution**: Pre-trained embedding models have extensive vocabularies and can handle a wide range of words. Additionally, models like FastText can generate embeddings for out-of-vocabulary words by using subword information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca829fb7-a9a1-4a05-b0c5-e6a868790082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84e45795-d648-4b16-8dc2-4de15a6d1e33",
   "metadata": {},
   "source": [
    "## Additional Advantages of Embedding Models\n",
    "\n",
    "1. **Transfer Learning**:\n",
    "   - Pre-trained embedding models like BERT can be fine-tuned on specific tasks with relatively small amounts of task-specific data, leveraging large-scale pre-training on vast corpora.\n",
    "\n",
    "2. **Efficient Representation**:\n",
    "   - Dense embeddings reduce memory usage and computational requirements compared to sparse representations from BoW models.\n",
    "\n",
    "3. **Handling Variable-Length Inputs**:\n",
    "   - Embeddings provide a fixed-size vector representation regardless of the input text length, making them suitable for downstream machine learning models that require fixed-size inputs.\n",
    "\n",
    "4. **Capturing Long-Range Dependencies**:\n",
    "   - Models like BERT and GPT, based on the Transformer architecture, are particularly effective at capturing long-range dependencies and relationships between words over long contexts.\n",
    "\n",
    "5. **Improved Performance on Downstream Tasks**:\n",
    "   - Embedding models have shown superior performance on a wide range of natural language processing tasks, including sentiment analysis, named entity recognition, machine translation, and question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff296265-1523-4e50-ae34-4aa0a1db528e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
